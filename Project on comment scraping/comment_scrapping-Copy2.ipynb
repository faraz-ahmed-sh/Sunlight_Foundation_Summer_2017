{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assignment: Open Data Policies comments scrape\n",
    "\n",
    "import re\n",
    "import util\n",
    "import bs4\n",
    "import queue\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import urllib\n",
    "import collections\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "import sys\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-00ff0b082a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '.', 'the', ',', 'and', 'of', ';', 'for', 'that', 'on', 'to', 'a', 'or', 'is', 'in', 'â€™', '\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starting_urls_list = [\"https://mymadison.io/documents/city-of-buffalo-open-data-policy\", \n",
    "                      \"https://mymadison.io/documents/durham-open-data-policy\",\n",
    "                      \"https://mymadison.io/documents/city-of-tyler-data-policy\",\n",
    "                      \"https://mymadison.io/documents/city-of-glendale-draft-open-data-resolution\",\n",
    "                      \"https://mymadison.io/documents/metro-nashville-government-open-data-policy\",\n",
    "                      \"https://mymadison.io/documents/city-of-syracuse-open-data-policy\",\n",
    "                      \"https://mymadison.io/documents/city-of-buffalo-open-data-policy\",\n",
    "                      \"https://mymadison.io/documents/napervilleopendatapolicy\",\n",
    "                      \"https://mymadison.io/documents/bart-open-data-policy\",\n",
    "                      \"https://mymadison.io/documents/san-francisco-open-data-legislation-2014\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starting_url = \"https://mymadison.io/documents/city-of-buffalo-open-data-policy?comment_page=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_crawl(starting_url):\n",
    "    '''\n",
    "    Scrapes the full webpage from a starting url.\n",
    "\n",
    "    Inputs:\n",
    "        starting_url: string\n",
    "    Returns:\n",
    "        a soup object\n",
    "    '''    \n",
    "    page = urllib.request.urlopen(starting_url)\n",
    "    if page is not None:\n",
    "        soup = bs4.BeautifulSoup(page, \"lxml\")\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comment_id(comment):\n",
    "    comment_id = comment.get('id')\n",
    "    return comment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name(comment):\n",
    "    name_date = comment.find(\"div\", class_=\"media-body media-middle\")\n",
    "    name = name_date.find(\"span\")\n",
    "    name_final = name.string.strip()\n",
    "    return name_final, name_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datetime(comment):\n",
    "    name_date = name(comment)[1]\n",
    "    time = name_date.find(\"time\")\n",
    "    datetime_final = time.get(\"datetime\")[:-6]\n",
    "    datetime_final_1 = datetime_final.replace(\"T\", \" \")\n",
    "    return datetime_final_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likes_count(comment):\n",
    "    name_date = name(comment)[1]\n",
    "    likes = name_date.find(\"span\", class_=\"action-count\")\n",
    "    return likes.string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quoted_comment(comment):\n",
    "    comments = comment.find(\"div\", class_=\"comment-content\")\n",
    "    quoted_comment = comments.find(\"blockquote\")\n",
    "    if quoted_comment is not None:\n",
    "        quoted_comment_final = quoted_comment.text.strip()\n",
    "        return quoted_comment_final, comments\n",
    "    else:\n",
    "        return None, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actual_comment(comment):\n",
    "    comments = comment.find(\"div\", class_=\"comment-content\")\n",
    "    actual_comments = comments.find_all(\"p\", recursive=False)\n",
    "    \n",
    "    complete_str_aux = ''\n",
    "    complete_str_str = ''\n",
    "    \n",
    "    for child in actual_comments:\n",
    "        complete_str_aux += str(child)\n",
    "        complete_str_str += str(child.text)\n",
    "    return complete_str_aux, complete_str_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reply_ids(comment):\n",
    "    \n",
    "    replies = comment.find_all(\"div\", class_=\"comment\")\n",
    "    if replies is not None:\n",
    "        reply_id_list = []\n",
    "        for reply in replies:\n",
    "            reply_id=reply.get('id')\n",
    "            reply_id_list.append(reply_id)\n",
    "\n",
    "        return reply_id_list\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_total_num_pages(starting_url):\n",
    "    soup = simple_crawl(starting_url)\n",
    "    next_page = soup.find_all(\"ul\", class_=\"pagination\")\n",
    "    \n",
    "    for page in next_page:\n",
    "        pages = page.find_all(\"a\")\n",
    "        total_num_pages = pages[-2].get(\"href\")\n",
    "        return int(total_num_pages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calling_functions(commentators, comments_dictionary, comment_number):\n",
    "    \n",
    "    for comment in commentators:\n",
    "        \n",
    "        comment_id_f = comment_id(comment)\n",
    "        name_f =  name(comment)\n",
    "        timedate_f = datetime(comment)\n",
    "        likes_count_f = likes_count(comment)\n",
    "        quoted_comment_f = quoted_comment(comment)\n",
    "        actual_comment_f, actual_comment_str =  actual_comment(comment)\n",
    "        reply_ids_f = reply_ids(comment)\n",
    "\n",
    "\n",
    "        comments_dictionary[comment_number] = [comment_id_f,name_f[0], timedate_f, \n",
    "                                      likes_count_f, quoted_comment_f[0],\n",
    "                                       actual_comment_str, actual_comment_f, reply_ids_f]\n",
    "        comment_number += 1\n",
    "    \n",
    "    return comments_dictionary, comment_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape a particular open data policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_all_pages(starting_url):\n",
    "    '''\n",
    "    This functions scrapes the required data from all the webpages for a PARTICULAR starting url of a Madison webpage.\n",
    "    '''\n",
    "    comments_dictionary = {}\n",
    "    num_pages_to_crawl = find_total_num_pages(starting_url)\n",
    "    \n",
    "    if num_pages_to_crawl is None:\n",
    "        num_pages_to_crawl = 1\n",
    "        \n",
    "    loop_number = 0\n",
    "    comment_number = 1\n",
    "\n",
    "    while loop_number < num_pages_to_crawl:\n",
    "    \n",
    "        soup = simple_crawl(starting_url)\n",
    "        commentators = soup.findAll(\"li\", class_=\"comment floating-card\")\n",
    "        \n",
    "        comments_dictionary, comment_number = calling_functions(commentators, comments_dictionary, comment_number)\n",
    "        \n",
    "        next_page = soup.find_all(\"ul\", class_=\"pagination\")\n",
    "        for page in next_page:\n",
    "            pages = page.find_all(\"a\")\n",
    "            starting_url = pages[-1].get(\"href\")\n",
    "        \n",
    "        loop_number += 1\n",
    "        \n",
    "    return comments_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_dictionary = scrape_all_pages(starting_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_dict_pandas(comments_dictionary):\n",
    "    \n",
    "    df = pd.DataFrame(comments_dictionary)\n",
    "    df = df.transpose()\n",
    "    df.columns = ['comment_id', 'author', 'datetime', 'num_likes', 'quoted_text', 'comment_text', 'comment_text_aux', 'reply_ids']\n",
    "\n",
    "    # converting str datetime into datetime pandas format\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_dict_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-953168fd4915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvert_dict_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_dict_pandas' is not defined"
     ]
    }
   ],
   "source": [
    "convert_dict_pandas(comments_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_dict_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-902c90bc9237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_dict_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_dict_pandas' is not defined"
     ]
    }
   ],
   "source": [
    "temp = convert_dict_pandas(comments_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e279ef1022a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# temp['quoted_text'].dropna().map(lambda y: y.replace('<p>','').replace('</p>','')).apply(lambda x: pd.Series(x).value_counts()).sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quoted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "# temp['quoted_text'].dropna().map(lambda y: y.replace('<p>','').replace('</p>','')).apply(lambda x: pd.Series(x).value_counts()).sum()\n",
    "temp['quoted_text'].dropna().apply(lambda x: pd.Series(x.lower().split()).value_counts()).sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) finding out most common words in quoted texts (policy sections) and actual comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -most common words in quoted texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list_qt = temp['quoted_text'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f198c1b84828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstring_quoted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quoted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "string_quoted_text = temp['quoted_text'].str.cat(sep=' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string_quoted_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-91caf266f729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_quoted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgood_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string_quoted_text' is not defined"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(string_quoted_text)\n",
    "good_words = [i for i in words if i not in stop] # stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'good_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-51ac36dac2e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'good_words' is not defined"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(good_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-005e5c29e0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon_quoted_text_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fdist' is not defined"
     ]
    }
   ],
   "source": [
    "common_quoted_text_20 = fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'common_quoted_text_20' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6e5f73a87017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon_quoted_text_20\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'common_quoted_text_20' is not defined"
     ]
    }
   ],
   "source": [
    "common_quoted_text_20[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-70a414e6da2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#resize the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fdist' is not defined"
     ]
    }
   ],
   "source": [
    "fdist.plot(20) #resize the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -most common words in actual comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d53ed912e883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstring_actual_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "string_actual_comments = temp['comment_text'].str.cat(sep=' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string_actual_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-953043f1efdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_actual_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgood_words_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_comments\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string_actual_comments' is not defined"
     ]
    }
   ],
   "source": [
    "words_comments = nltk.tokenize.word_tokenize(string_actual_comments)\n",
    "good_words_comments = [i for i in words_comments if i not in stop] # stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'good_words_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-17fd500d82bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfdist_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_words_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'good_words_comments' is not defined"
     ]
    }
   ],
   "source": [
    "fdist_comments = FreqDist(good_words_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7807e672803b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon_comment_text_30\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdist_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fdist_comments' is not defined"
     ]
    }
   ],
   "source": [
    "common_comment_text_30 = fdist_comments.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'common_comment_text_30' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-80a33bc548e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon_comment_text_30\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'common_comment_text_30' is not defined"
     ]
    }
   ],
   "source": [
    "common_comment_text_30[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fd076f275ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfdist_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#resize the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fdist_comments' is not defined"
     ]
    }
   ],
   "source": [
    "fdist_comments.plot(30) #resize the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) finding out most common quoted text n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-661011fb85c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstring_quoted_text_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quoted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "string_quoted_text_list = temp['quoted_text'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string_quoted_text_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-39ad3b97a72a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#reader = csv.reader(string_quoted_text, delimiter=\",\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#reader.next()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_quoted_text_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mextract_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string_quoted_text_list' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    " \n",
    "from collections import Counter\n",
    " \n",
    "non_speaker = re.compile('[A-Za-z]+: (.*)')\n",
    " \n",
    "def extract_phrases(text, phrase_counter, length):\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        strip_speaker = non_speaker.match(sent)\n",
    "        if strip_speaker is not None:\n",
    "            sent = strip_speaker.group(1)\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        for phrase in ngrams(words, length):\n",
    "            phrase_counter[phrase] += 1\n",
    " \n",
    "phrase_counter = Counter()\n",
    " \n",
    "#with open(\"data/import/sentences.csv\", \"r\") as sentencesfile:\n",
    "#reader = csv.reader(string_quoted_text, delimiter=\",\")\n",
    "#reader.next()\n",
    "for sentence in string_quoted_text_list:\n",
    "    extract_phrases(sentence, phrase_counter, 4)\n",
    " \n",
    "most_common_phrases = phrase_counter.most_common(15)\n",
    "\n",
    "for k,v in most_common_phrases:\n",
    "    print ('{0: <5}'.format(v), k)\n",
    "\n",
    "# decide on the limit of words frequency (4?)\n",
    "# now do a jaro wrinkler score to bring the text together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding out negative and positive sentiments for popular policy sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'most_common_phrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a0ba4259ffc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_common_phrases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstring_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_phrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(temp[temp['quoted_text'].astype(str).str.contains(string_phrase)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'most_common_phrases' is not defined"
     ]
    }
   ],
   "source": [
    "for phrase in most_common_phrases:\n",
    "    string_phrase = ' '.join(phrase[0])\n",
    "    print(string_phrase)\n",
    "\n",
    "    #print(temp[temp['quoted_text'].astype(str).str.contains(string_phrase)])\n",
    "    #print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-214b2c66b65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactual_comment_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quoted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Open Data Governance Committee'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomment_list_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactual_comment_filter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "actual_comment_filter = temp[temp['quoted_text'].astype(str).str.contains('Open Data Governance Committee')]\n",
    "comment_list_sentiment = actual_comment_filter['comment_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 'governance', 'committee'), ('open', 'data', 'governance'), ('the', 'open', 'data')]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# change this to read in your data\n",
    "finder = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "# only trigrams that appear 3+ times\n",
    "finder.apply_freq_filter(4) \n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.pmi, 20))\n",
    "\n",
    "#for k,v in finder.ngram_fd.items():\n",
    "#  print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NgramQueryWeights= [('the', 35), ('data', 30), ('and', 27), ('of', 22), ('open data', 15), ('open', 15), ('for', 14), ('that', 13), ('to', 11), ('on', 11), ('or', 8), ('the open data', 7), ('the open', 7), ('city', 7), ('public', 7), ('open data governance committee', 6), ('data governance committee', 6), ('open data governance', 6), ('governance committee', 6), ('data governance', 6), ('committee', 6), ('governance', 6), ('non', 6), ('of the', 5), ('the public', 5), ('in', 5), ('any', 5), ('policy', 5), ('is', 5), ('records', 5), ('information', 5), ('the open data governance committee', 4), ('the open data governance', 4), ('the data', 4), ('policy concerns', 4), ('develop and', 4), ('implementation', 4), ('concerns', 4), ('access', 4), ('ensure', 4), ('use', 4), ('develop', 4), ('datasets', 4), ('on internal and external needs', 3), ('based on internal and external', 3), ('data based on internal and', 3), ('the data based on internal', 3), ('update the data based on', 3), ('periodically update the data based', 3), ('non privileged and non confidential', 3), ('internal and external needs', 3), ('on internal and external', 3), ('based on internal and', 3), ('data based on internal', 3), ('the data based on', 3), ('update the data based', 3), ('periodically update the data', 3), ('privileged and non confidential', 3), ('non privileged and non', 3), ('on use or reuse', 3), ('and external needs', 3), ('internal and external', 3), ('on internal and', 3), ('based on internal', 3), ('data based on', 3), ('the data based', 3), ('update the data', 3), ('periodically update the', 3), ('open data portal', 3), ('and non confidential', 3), ('privileged and non', 3), ('non privileged and', 3), ('use or reuse', 3), ('on use or', 3), ('including the records', 3), ('from the public', 3), ('in the', 3), ('on the', 3), ('external needs', 3), ('and external', 3), ('internal and', 3), ('on internal', 3), ('based on', 3), ('data based', 3), ('update the', 3), ('periodically update', 3), ('data that', 3), ('to the', 3), ('data is', 3), ('ensure that', 3), ('data portal', 3), ('non confidential', 3), ('and non', 3), ('privileged and', 3), ('non privileged', 3), ('or reuse', 3), ('use or', 3), ('on use', 3), ('the records', 3), ('including the', 3)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "string_quoted_text_1 = string_quoted_text\n",
    "vect = CountVectorizer(ngram_range=(1,5))\n",
    "analyzer = vect.build_analyzer()\n",
    "listNgramQuery = analyzer(string_quoted_text_1)\n",
    "listNgramQuery.reverse()\n",
    "#print (\"listNgramQuery=\", listNgramQuery)\n",
    "NgramQueryWeights = nltk.FreqDist(listNgramQuery)\n",
    "print (\"\\nNgramQueryWeights=\", NgramQueryWeights.most_common(100))\n",
    "\n",
    "#temp[].dropna().apply(lambda x: pd.Series(x.lower().split()).value_counts()).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape all open data policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def go_all_madison_websites(starting_urls_list):\n",
    "    '''\n",
    "    This function scrapes the comments from all the Madison related webistes for cities that have\n",
    "    launched open data policies and converts them into pandas dataframe objects.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame object for each city's open data policy.\n",
    "    '''\n",
    "    \n",
    "    all_madison_websites_dfs = []\n",
    "    for starting_url in starting_urls_list:\n",
    "        #print(starting_url)\n",
    "        comments_dictionary = scrape_all_pages(starting_url)\n",
    "        get_dict_to_pandas = convert_dict_pandas(comments_dictionary)\n",
    "        relative_url = starting_url.split('/')[-1]\n",
    "        all_madison_websites_dfs.append((relative_url, get_dict_to_pandas))\n",
    "        \n",
    "    return all_madison_websites_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_madison_dfs = go_all_madison_websites(starting_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for city_name, df in all_madison_dfs:\n",
    "#    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_tags(starting_url):\n",
    "    soup = simple_crawl(starting_url)\n",
    "    commentators = soup.findAll(\"li\", class_=\"comment floating-card\")    \n",
    "    \n",
    "    for comment in commentators:\n",
    "        \n",
    "        #------------finding names-------------#\n",
    "        \n",
    "        name_date = comment.find(\"div\", class_=\"media-body media-middle\")\n",
    "        #print(type(names))\n",
    "        #break\n",
    "        \n",
    "        #for name in names:\n",
    "        #    print(name)\n",
    "        \n",
    "        name = name_date.find(\"span\")\n",
    "        #print(name.string.strip())\n",
    "        \n",
    "        #for name in names:\n",
    "        \n",
    "        #------------finding datetime-------------#\n",
    "        \n",
    "        time = name_date.find(\"time\")\n",
    "        time_update = time.get(\"datetime\")[:-6]\n",
    "        \n",
    "        #time_update_1 = time_update.replace((/[A-Z]/), ' ')\n",
    "        time_update_1 = time_update.replace(\"T\", \" \")\n",
    "        #print(time_update_1)\n",
    "         \n",
    "        \n",
    "        #------------finding comment_id-------------#\n",
    "        \n",
    "        #comment_id=comment.find(\"li\").extract()\n",
    "        comment_id=comment.get('id')\n",
    "        #print(comment_id)\n",
    "        \n",
    "        #------------finding likes(if any)-------------#\n",
    "        \n",
    "        likes = name_date.find(\"span\", class_=\"action-count\")\n",
    "        #print(likes.string.strip())\n",
    "        \n",
    "        #------------finding quoted comment-------------#\n",
    "        \n",
    "        comments = comment.find(\"div\", class_=\"comment-content\")\n",
    "        quoted_comment = comments.find(\"blockquote\")\n",
    "        if quoted_comment is not None:\n",
    "            quoted_comment_final = quoted_comment.text\n",
    "            #print(quoted_comment_final)\n",
    "    \n",
    "        \n",
    "        #------------finding actual comment-------------#\n",
    "        comments = comment.find(\"div\", class_=\"comment-content\")\n",
    "        #children = comments.children\n",
    "        children = comments.find_all(\"p\", recursive=False)\n",
    "        #print(children)\n",
    "        \n",
    "        complete_str = ''\n",
    "        \n",
    "        for child in children:\n",
    "            #print(child)\n",
    "            complete_str += str(child)\n",
    "        print(\"STR\",complete_str)\n",
    "            \n",
    "        #number = 1\n",
    "        #for actual_comment in children:\n",
    "            #print(\"ACT \" + str(number), actual_comment)\n",
    "            \n",
    "            #if len(actual_comment) > 1:\n",
    "            #    print(actual_comment)\n",
    "            #else:\n",
    "                #print(actual_comment)\n",
    "            #number += 1\n",
    "        \n",
    "        \n",
    "        #for child in children:\n",
    "        #    print(type(child))\n",
    "            \n",
    "            \n",
    "            \n",
    "            #actual_comment = child.string\n",
    "            #print(actual_comment)\n",
    "            #if actual_comment is not None:\n",
    "                \n",
    "                #actual_comment_final = actual_comment\n",
    "                #print(actual_comment_final)\n",
    "                \n",
    "                \n",
    "        #------------finding reply(or replies') id(s)-------------#\n",
    "        \n",
    "        replies = comment.find_all(\"div\", class_=\"comment\")\n",
    "        if replies is not None:\n",
    "            for reply in replies:\n",
    "                reply_id=reply.get('id')\n",
    "                #print(reply_id)\n",
    "        \n",
    "    #------------finding the next page to crawl-------------#\n",
    "    \n",
    "    next_page = soup.find_all(\"ul\", class_=\"pagination\")\n",
    "    \n",
    "    \n",
    "    for i in next_page:\n",
    "        pages = i.find_all(\"a\")\n",
    "        #print(pages)\n",
    "        #print(pages[-1].get(\"href\"))\n",
    "        \n",
    "        num_pages = pages[-2].get(\"href\")\n",
    "        #print(num_pages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STR <p>I am in support of the proposed policy to further enable data and subsequent knowledge sharing. I agree with fellow contributors that it would be beneficial to see more detailed outlines for the specific process that is going to enable community members to contribute to the Open Data portal by creating, uploading and verifying data sets to be published. Myself and colleagues have created a number of valuable data sets that could be beneficial across city departments, community organizations and individual interests. Understanding that only data sets meeting the standards set forth as a result of this policy will be accepted, we can collaboratively drive the understanding of our community from all perspectives.</p>\n",
      "STR <p>I would be interested to see this more finely documented. It would be great to have a living, breathering process that community members could submit data sets that they have created (required to meet the data standards set forth by the Open Data Policy) for verification and publication to the open data portal within a timely manner. Individuals in the community may have greater flexibility and time to produce unique data sets pertinent to acute challenges and enable broader understanding in a more timely manner.</p>\n",
      "STR <p>I strongly support the proposed policy, with a recognition of the outstanding contributions that have already been made by other residents, citizens, and interested parties.</p><p>Many thanks to the City of Buffalo for leading this effort. Most of my comments are within the document itself, but I do agree with other comments below that emphasize the need for substantial citizen involvement in governance, application, and oversight. During the RFP process, I would encourage the city to include not only requirements for a portal, but also an iterative feedback/improvement process with residents and regular data users so that the city's investment in this system can produce tangible use cases and real benefits to our city and its people.</p><p>The Department of Citizen Services should also be commended for their use of the collaborative policymaking platform. In the age of the internet, this is an excellent way to increase public participation in these types of processes. Many thanks for this opportunity.</p>\n",
      "STR <p>Can the portal include an option for community members/partners to publish their own data to a section of the site? Provided it adheres to a set of minimum standards and passes review.</p>\n",
      "STR <p>It may be a helpful option to include a repository of FOIL requests and responses (whatever is able to be centralized), as well as an option to automate or streamline the FOIL request process via the portal interface.</p>\n",
      "STR <p>Minutes from any and all meetings of this committee should be made available in machine-readable formats in an easily-accessible location (including within the open data system)</p>\n",
      "STR <p>A Citizens Advisory Committee can be designated (appointments by mayor, council members) to support the Governance Committee in the areas of reviewing datasets to be published, hosting events for public feedback and participation, facilitating collaborations, facilitating use of the portal, expanding awareness of open data resources and applications, as well as defining and regularly reviewing the data standards governance document.</p>\n",
      "STR <p>equitable access to opportunity (open data can help move us closer to this goal)</p>\n",
      "STR <p>This part of the reporting should make specific reference to the three goals articulated in Section \"Open Data Program\", part 6: government efficiency, improved quality of life, and expanded economic opportunities.</p>\n",
      "STR <p>The open data system should include a space for showcasing the innovative ways in which city data are used by these various stakeholders.</p>\n",
      "STR <p>Remove \"paramount.\" Privacy, confidentiality and security should be considerations when determining whether data should be open, but they should not be presumed to be paramount and therefore superior to the public's right to know.</p>\n",
      "STR <p>I strongly support the spirit of this policy and the efforts made toward ensuring free public access to data information and government transparency. As a private citizen and freelance writer and researcher, I support the timely adoption and implementation of this policy.</p><p>The following are my concerns with the policy as it now stands:</p><p>The policy should encourage and, when possible, mandate the collection and dissemination of information collected by non-city and quasi-governmental agencies such as the NFTA, BMHA, Buffalo Place, the Buffalo Olmsted Parks Conservancy, hospital campuses, institutions of higher education and other organizations providing public services such as park maintenance, policing and security, transportation and education services. In particular, public safety information from police and security forces managed by Buffalo Place, medical and college campuses and the NFTA should be subject to the open data policy.</p><p>When information is unavailable due to its privileged status, it should be made available in the aggregate whenever possible. e.g., motor vehicle incident reports are not public per NYS law but should be presented to the public in an anonymized or aggregate form.</p>\n",
      "STR <p>The following comment is submitted on behalf of the Public Accountability Initiative:</p><p>The Public Accountability Initiative (PAI) is a non-profit educational group focused on transparency and accountability headquartered in the City of Buffalo. PAI operates a research platform, LittleSis.org, that brings together information obtained through public records for use by journalists, academics, and activists as well as the general public. Access to current government information is paramount to PAIâ€™s work and to meaningful participation in a democracy. As such, PAI endorses the goals of the Open Data proposal.</p><p>Below we have highlighted what feel are the most commendable inclusions in the proposal as currently drafted, made suggestions for further improvement, and raised concerns with certain provisions of the proposal.</p><p>By providing access to public information proactively and for free, the Open Data proposal addresses two significant burdens to public access to information: failure to respond to freedom of information requests in a timely fashion and undue copying fees levied by public agencies. According to MuckRock, a non-profit organization that facilitates the filing and analysis of freedom of information requests, the City of Buffalo takes an average of 63 days to respond to freedom of information requests and charges an average fee of $4.13 per request. PAI has had considerable difficulty obtaining public records from city agencies, most notably the Buffalo Police Department (see e.g. â€œFOIL by inches: The slow-drip transparency of Buffalo PDâ€ by Caitlin Russell of MuckRock).</p><p>If these burdens are truly shifted from requesters to the City, Buffalo will be making a commendable commitment to transparency. To even further improve access to public records, PAI seconds Brian Borncampâ€™s recommendation that the Open Data Policy provide for a â€œcentralized system for submitting, tracking, and responding to FOIL requests.â€ By centralizing all requests and by making requests and responses publicly available, the City can dramatically increase its responsiveness to public requests for information and reduce instances of non-compliance with the Freedom of Information Law that result in costly attorneysâ€™ fees.</p><p>PAI also highlights and applauds the inclusion of â€œrecords of third-party agency contractors that create or acquire information, records, or data on behalf of a City division/departmentâ€ as an especially meaningful and important component of the Open Data proposal. Public entities are increasingly reliant on non-governmental or quasi-governmental non-profits to perform governmental services. If unchecked, those third-party contractors can act as a â€œblack boxâ€ where important information can be kept secret from the public, leading to suspicion and, at times, scandal. The State University at New York system is currently dealing with this issue within its nominally private campus-related foundations at the center of the United States Attorney investigation into Buffalo Billion contracting.</p><p>Explicitly holding third-party contractors to the same transparency standard as the public agencies theyâ€™re working for both honors the letter of New Yorkâ€™s Freedom of Information Law and serves as a proactive measure to avoid actual or apparent corruption. This provision should be taken further to explicitly include records of all city-related public benefit corporations and other quasi-governmental entities.</p><p>Finally, as PAI is involved with technology development in addition to producing original research, we call attention to the proposed availability of City of Buffalo data via an Application Programming Interface (API). Providing data this way allows developers to interact with data in new and unexpected ways and facilitates analysis of that data. For example, PAIâ€™s research platform LittleSis interacts with the OpenSecrets database maintained by the Center for Responsive Politics via an API to incorporate campaign finance as soon as it is uploaded. In addition to making City of Buffalo data available via an API, PAI also recommends that datasets be available for bulk download in various relevant formats to permit analysis using Microsoft Excel or other programs.</p><p>PAI has significant concerns about the oversight of Buffaloâ€™s Open Data portal. As currently stipulated in the proposal, development and implementation of the Open Data practices will be delegated to appointees of the Mayor with â€œinput from the public.â€ It is crucial for the Open Data Governance Committee to be free from the influence of politics and to not be beholden to any one political representative. As such, the Open Data Governance Committee should include representatives from the City as well as from independent organizations working in the public interest, such as the residents, businesses, researchers, and media mentioned in the policyâ€™s statement of purpose, and independent members should comprise a majority of the committeeâ€™s votes.</p><p>Alongside the existing Open Book Buffalo portal, this Open Data policy represents a praiseworthy commitment to transparency and accountability for the City of Buffalo. For this, the Public Accountability Initiative again commends the city government and welcomes any further opportunity for input on this important initiative. By adopting a policy that addresses the above-stated concerns addressed â€“ especially those about oversight and the centralization of freedom of information requests and responses â€“ the City of Buffalo will take a great stride in advancing transparency and accountability to its residents.</p>\n",
      "STR <p>I assume this would be funding for training the support staff.  I would like to also recommend that a portion of the funding be allocated for training residents, community groups and nonprofits on how to access information and analyze it to inform their work.</p>\n",
      "STR <p>It is important to include all members of the local community that can benefit from this information; including nonprofits and foundations.</p>\n"
     ]
    }
   ],
   "source": [
    "name_tags(starting_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
